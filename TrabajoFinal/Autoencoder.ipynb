{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRYEofSD0xoF"
   },
   "source": [
    "# PyTorch: Autoencoder convolucional Fashion-MNIST\n",
    "\n",
    "## Refs.\n",
    "\n",
    "* https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "\n",
    "* https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "* https://github.com/pranay414/Fashion-MNIST-Pytorch/blob/master/fashion_mnist.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDsSrjkCoG5q"
   },
   "source": [
    "**Idea del autoencoder**\n",
    "\n",
    "* Entrada: una imagen.\n",
    "* Salida: la misma imagen, suele llamarse **reconstrucción**.\n",
    "\n",
    "La función de costo contiene una \"pérdida de reconstrucción\", que penaliza al modelo cuando la reconstrucción es muy distinta del input.\n",
    "\n",
    "En la capa del medio (una vez que ya se hizo una reducción de dimensionalidad o compresión gracias a poolings y convoluciones), se tiene una idea de cuáles son las features más importantes de la imagen de entrada.\n",
    "\n",
    "Al hacer la reducción de dimensionalidad, el model aprende una \"pseudo-identidad\": aproxima la identidad solamente sobre el conjunto de datos.\n",
    "\n",
    "Encoder: zip, Decoder: unzip.\n",
    "\n",
    "O sea, los autoencoders son capaces de aprender representaciones eficientes de los datos de entrada, llamadas **codificaciones**.\n",
    "\n",
    "**Convolución (o filtros):** sirven para reducir \"eficientemente\" la dimensión de los datos cuya representación \"guarda\" algún tipo de estructura \"espacial\".\n",
    "\n",
    "Importante de las convoluciones: genera varias imágenes a partir de una sola. Cada convolución se concentra en un aspecto particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LEtbxmgoGCh"
   },
   "source": [
    "## **Ejercicio 1)** Importando librerías\n",
    "\n",
    "**0)** De ser necesario, **instale PyTorch** escribiendo\n",
    "\n",
    "    !pip3 install torch torchvision torchaudio torchviz\n",
    "\n",
    "**1)** Importe las librerías estandard de Python: `os`, `datetime`, `collections` y `pickle`.\n",
    "\n",
    "**2)** Importe las siguientes librerías third party de Python: `matplotlib.pyplot`, `numpy`, `scipy`, `sklearn`, `pandas`, `dill` y `json`.\n",
    "\n",
    "**3)** Importe las librerias necesarias de **PyTorch**: `torch` y `torchvision`.\n",
    "\n",
    "**4)** Importe la librería: `google.colab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12048,
     "status": "ok",
     "timestamp": 1732223840371,
     "user": {
      "displayName": "BENJAMÍN BAS PERALTA",
      "userId": "06928385369853535922"
     },
     "user_tz": 180
    },
    "id": "Jg3VSqHCGSub",
    "outputId": "9e576120-0d7f-4671-93d3-e16b0a5c07e4"
   },
   "outputs": [],
   "source": [
    "# 1.0)\n",
    "!pip3 install torch torchvision torchaudio torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1732223840371,
     "user": {
      "displayName": "BENJAMÍN BAS PERALTA",
      "userId": "06928385369853535922"
     },
     "user_tz": 180
    },
    "id": "I8N3D_nU1_oT"
   },
   "outputs": [],
   "source": [
    "# 1.1)\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5562,
     "status": "ok",
     "timestamp": 1732223845931,
     "user": {
      "displayName": "BENJAMÍN BAS PERALTA",
      "userId": "06928385369853535922"
     },
     "user_tz": 180
    },
    "id": "QsfFvPYhkCGl"
   },
   "outputs": [],
   "source": [
    "# 1.2)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.linalg as linalg\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "#import dill\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19388,
     "status": "ok",
     "timestamp": 1732223865317,
     "user": {
      "displayName": "BENJAMÍN BAS PERALTA",
      "userId": "06928385369853535922"
     },
     "user_tz": 180
    },
    "id": "Uot5sVNnkCNa"
   },
   "outputs": [],
   "source": [
    "# 1.3)\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "#from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVCiYt-1kCUi"
   },
   "outputs": [],
   "source": [
    "# 1.4)\n",
    "# import google.colab\n",
    "# from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1732223865318,
     "user": {
      "displayName": "BENJAMÍN BAS PERALTA",
      "userId": "06928385369853535922"
     },
     "user_tz": 180
    },
    "id": "oUFvWw_kr7Bt",
    "outputId": "541748b4-7b58-4f68-d4ac-dd47d1b3df85"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcaGEHAd10sb"
   },
   "source": [
    "## **Ejercicio 2)**\n",
    "\n",
    "Bajando y Jugando con el dataset **Fashion-MNIST**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gDYWwg-lhsC"
   },
   "source": [
    "**1)** Baje y transforme los conjuntos de entrenamiento y testeo de FashionMNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6927,
     "status": "ok",
     "timestamp": 1732223945595,
     "user": {
      "displayName": "BENJAMÍN BAS PERALTA",
      "userId": "06928385369853535922"
     },
     "user_tz": 180
    },
    "id": "NUoQ9bnwaZ7O",
    "outputId": "c50ccca2-a3db-43bf-8877-0576a829e20b"
   },
   "outputs": [],
   "source": [
    "# 2.1)\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load the training data\n",
    "train_set_orig = datasets.FashionMNIST('MNIST_data/', download = True, train = True,  transform = transform)\n",
    "valid_set_orig = datasets.FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 550,
     "status": "ok",
     "timestamp": 1732224170592,
     "user": {
      "displayName": "BENJAMÍN BAS PERALTA",
      "userId": "06928385369853535922"
     },
     "user_tz": 180
    },
    "id": "lsog4cT-msxX",
    "outputId": "4adb0439-5486-420c-ee9b-f72219475210"
   },
   "outputs": [],
   "source": [
    "labels_names = dict(enumerate(train_set_orig.classes))\n",
    "labels_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwUfjL0Ylh2P"
   },
   "source": [
    "**2)** Grafique un mosaico de 3x3 imagenes de FashionMNIST, cada una titulada con su respectiva clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "executionInfo": {
     "elapsed": 897,
     "status": "ok",
     "timestamp": 1732224227542,
     "user": {
      "displayName": "BENJAMÍN BAS PERALTA",
      "userId": "06928385369853535922"
     },
     "user_tz": 180
    },
    "id": "-wJdl9mKx5EC",
    "outputId": "24b7bff4-dfda-4988-e112-d4acbd3aaa0d"
   },
   "outputs": [],
   "source": [
    "# 2.2)\n",
    "figure = plt.figure()\n",
    "cols,rows = 3,3\n",
    "for i in range(1,cols*rows+1):\n",
    "    j = torch.randint(len(train_set_orig),size=(1,)).item() # Los números aleatorios tambien se pueden generar desde pytorch. Util para trabajar en la GPU.\n",
    "    image,label = train_set_orig[j]\n",
    "    figure.add_subplot(rows,cols,i)\n",
    "    plt.title(f\"{labels_names[label]} (Class {label})\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image.squeeze(),cmap=\"Greys_r\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OWYnfxWz8RS"
   },
   "source": [
    "## Ejercicio 3) Creando un `DataSet` personalizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8k1_N7qmnL_A"
   },
   "source": [
    "**1)** Con el fin de implementar un autoencoder, cree una clase derivada de la clase `DataSet` (llámela, por ejemplo `CustomDataset`) que, en vez de retornal el label asociado a cada imagen de `FashionMNIST`, retorne la imagen misma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPjO1_Av1f87"
   },
   "outputs": [],
   "source": [
    "# 3.1)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = output = self.dataset[idx][0]\n",
    "        return input, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDyrC7C2nIy2"
   },
   "source": [
    "**2)** Utilice dicha clase para transformar los conjuntos de entrenamiento y testeo de `FashionMNIST` pensados para clasificación, a correspondientes conjuntos pensados para entrenar un autoencoder.\n",
    "Para ello, defina una clase `CustomDataset` que deriva de la clase `Dataset`, cuyo método `__getitem__(self,i)` retorne el pair `input,output` donde tanto `input` cómo `output` son iguales a la $i$-ésima imagen del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(train_set_orig)\n",
    "valid_set = CustomDataset(valid_set_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(train_set))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REToccG127zI"
   },
   "source": [
    "## Ejercicio 4) Red Neuronal Autoencoder Convolucional\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xEZge0lnNgb"
   },
   "source": [
    "**1)** Defina y cree una red neuronal *autoenconder convolucional* constituida por las siguientes capas:\n",
    "\n",
    "1. Una capa convolucional 2D constituida por:\n",
    "\n",
    "* Una capa `Conv2d` (ver https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) que deberá mapear $1$ canal de entradas de dimensiones $(28,28)$ a $16$ canales de de salida de dimensiones $(26,26)$. Para ello utilice un kernel de dimensiones $(3,3)$ y el resto de los parámetros en sus valores por defecto.\n",
    "* Una capa `ReLU`.\n",
    "* Una capa `Dropout`.\n",
    "* Una capa `MaxPool` (ver https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) con un kernel de dimensiones $(2,2)$, de manera que mapeará entradas de dimensiones $(26,26)$ a salidas de dimensiones $(13,13)$.\n",
    "\n",
    "2. Una capa lineal constituida por:\n",
    "\n",
    "* Una capa `Flatten` que mapea una entrada de $16$ canales de dimensiones $(13,13)$ a un vector de dimensión $16\\times 13\\times 13$.\n",
    "* Una capa `Linear` que mapea un vector de dimensión $16\\times 13\\times 13$ a un vector de dimensión $n$.\n",
    "* Una capa `ReLU`.\n",
    "* Una capa `Dropout`.\n",
    "\n",
    "3. Una capa capa convolucional 2D transpuesta, constituida por:\n",
    "\n",
    "* una capa `ConvTranspose2d` (ver https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) que mapea $16$ canales de dimensiones $(13,13)$ a $1$ canal de dimensiones $(28,28)$. Para ello, utilice un kernel de dimensiones $(6,6)$ y un stride de dimensiones $(2,2)$, dejando el resto de los parámetros en sus valores por defecto, i.e. padding de $(1,1)$ y out_padding de $(0,0)$.\n",
    "\n",
    "* Una capa `Sigmoid`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota sobre la función `Conv2d`:\n",
    "* Input: $N, C_{in}, H_{in}, W_{in}$\n",
    "* Output: $N, C_{out}, H_{out}, W_{out}$,\n",
    "\n",
    "donde:\n",
    "$$\n",
    "H_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{{padding}}[0] - \\text{{dilation}}[0] \\times (\\text{{kernel\\_size}}[0] - 1) - 1}{\\text{{stride}}[0]} + 1\\right\\rfloor\n",
    "\\\\\n",
    "\\\\\n",
    "W_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{{padding}}[1] - \\text{{dilation}}[1] \\times (\\text{{kernel\\_size}}[1] - 1) - 1}{\\text{{stride}}[1]} + 1\\right\\rfloor\n",
    "$$\n",
    "\n",
    "Signficado:\n",
    "* `padding`: padding added to all four sides of the input. Por defecto, `padding=0`\n",
    "* `dilation`: spacing between kernel elements. Por defecto, `dilation=1`.\n",
    "* `stride`: stride of the convolution. Por defecto, `stride=1`.\n",
    "\n",
    "Como nosotros, vamos a usar los parámetros por defecto para `padding`, `dilation` y `stride`, entonces las fórmulas de arriba se simplfican a:\n",
    "$$\n",
    "H_{out} = H_{in} - (\\text{{kernel\\_size}}[0] - 1)\n",
    "\\\\\n",
    "\\\\\n",
    "W_{out} = W_{in} - (\\text{{kernel\\_size}}[1] - 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3,3))\n",
    "max_pool = nn.MaxPool2d(kernel_size=(2,2))\n",
    "flattener = nn.Flatten(start_dim=1)\n",
    "linear = nn.Linear(16*13*13, 16*13*13)\n",
    "unflattener = nn.Unflatten(1, (16,13,13))\n",
    "conv_trans = nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=6, stride=2, padding=1)\n",
    "\n",
    "orig_img = (train_set[0][0]).unsqueeze(1)\n",
    "print(f\"Original image shape:      {orig_img.shape}\")\n",
    "img = conv(orig_img)\n",
    "print(f\"Convoluted image shape:    {img.shape}\")\n",
    "img = max_pool(img)\n",
    "print(f\"Max pooled img shape:      {img.shape}\")\n",
    "img = flattener(img)\n",
    "print(f\"Flattened image shape:     {img.shape}\")\n",
    "img = linear(img)\n",
    "print(f\"Linearized image shape:    {img.shape}\")\n",
    "img = unflattener(img)\n",
    "print(f\"Unflattened image shape:   {img.shape}\")\n",
    "img = conv_trans(img)\n",
    "print(f\"ConvTranspose image shape: {img.shape}\")\n",
    "\n",
    "print()\n",
    "print(f\"resulting_img.shape == final_img.shape: {orig_img.shape == img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJ0vazXTnWNK"
   },
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3),    # (1,28,28) -> (16,26,26)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=p),\n",
    "            nn.MaxPool2d(kernel_size=2)    # (16,26,26) -> (16,13,13)\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Flatten(),    # (16,13,13) -> 16x13x13\n",
    "            nn.Linear(in_features=16*13*13, out_features=16*13*13),    # 16x13x13 -> 16x13x13\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=p)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(dim=1, unflattened_size=(16,13,13)),    # 16x13x13 -> (16,13,13)\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=6, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTU2LRZenNkg"
   },
   "source": [
    "**2)** Grafique, a modo de comparación, unas pocas imagenes a predecir vs las correspondientes imagenes predichas utilizando el modelo sin entrenar y dropout $p=0.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSJqCozXCEq1"
   },
   "outputs": [],
   "source": [
    "def plot_orig_predicted(model, train_set, num_samples=3):\n",
    "    model_device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    num_samples = 2\n",
    "    fig, axes = plt.subplots(nrows=num_samples, ncols=2, figsize=(8, 2*num_samples))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        sample_idx = torch.randint(len(train_set), size=(1,)).item()\n",
    "        input = train_set[sample_idx][0].unsqueeze(1).to(model_device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input).squeeze(1).cpu().numpy()\n",
    "\n",
    "        # Plot original image\n",
    "        ax_orig = axes[i, 0]\n",
    "        ax_orig.imshow(input.squeeze().cpu().numpy(), cmap='gray')\n",
    "        ax_orig.axis('off')\n",
    "        ax_orig.set_title(f\"Original {sample_idx}\")\n",
    "        \n",
    "        # Plot predicted image\n",
    "        ax_pred = axes[i, 1]\n",
    "        ax_pred.imshow(output.squeeze(), cmap='gray')\n",
    "        ax_pred.axis('off')\n",
    "        ax_pred.set_title(f\"Reconstructed {sample_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvAutoencoder(p=0.2)\n",
    "plot_orig_predicted(model, train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9uINlg69OTw"
   },
   "source": [
    "## Ejercicio 5) Entrenando el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Implemente, en una función, un loop de entrenamiento que recorra los batchs (lotes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, dataloader, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    Trains the model for ONE epoch = goes over the dataset one time using the batches.\n",
    "    \"\"\"\n",
    "    num_batches = len(dataloader)\n",
    "    total_loss = 0\n",
    "    \n",
    "    model_device = next(model.parameters()).device\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(model_device), y.to(model_device)\n",
    "\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagate the prediction loss\n",
    "        # PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    "        loss.backward()\n",
    "\n",
    "        # Once we have our gradients, we call optimizer.step() to adjust the parameters\n",
    "        # by the gradients collected in the backward pass.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Reset the gradients of model parameters for the next batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Batch {batch} of {num_batches}. Loss in batch: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Implemente, en una función, un loop de prueba o validación que recorra los batchs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTAR: como no es un problema de clasificación, no hay accuracy. Está solamente\n",
    "# el valor de la pérdida\n",
    "\n",
    "def validate_step(model, dataloader, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    total_loss = 0\n",
    "\n",
    "    model_device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(model_device), y.to(model_device)\n",
    "\n",
    "            y_pred = model(X)\n",
    "            total_loss += loss_fn(y_pred, y).item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** Inicialize dos `DataLoader`s llamados `train_loader` y `valid_loader` que estén definidos sobre  el `train_set` (conjunto de entranmiento) y el `valid_set` (conjunto de prueba) de Fashion-MNIST, respectivamente, y que usen batchs de 100 ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(train_set_orig)\n",
    "valid_set = CustomDataset(valid_set_orig)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Cantidad de lotes de entrenamiento: {len(train_loader)}\")\n",
    "print(f\"Cantidad de lotes de validación:    {len(valid_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** Cree una función de pérdida usando el **Error Cuadrático Medio**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5)** Cree una instancia del modelo con dropout $p=0.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.2\n",
    "model = ConvAutoencoder(p=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6)** Cree un optimizador con un learning rate igual a $10^{-3}$.\n",
    "Pruebe con **ADAM**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 10e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7)** Especifique en que dispositivo (`device`) va a trabajar: en una **CPU** o en una **GPU**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8)** Implemente un loop que itere sobre épocas de entrenamiento y validación, y que guarde en listas correspondientes los siguientes valores del **ECM**:\n",
    "*  promedios (incorrectos) sobre el conjunto de entrenamiento, calculado **durante** el proceso de entrenamiento sobre la época.\n",
    "*  promedios (correctos) sobre el conjunto de entrenamiento, calculados **posteriormente** al proceso de entrenamiento sobre la época.\n",
    "*  promedios (correctos) sobre el conjunto de validación, calculados **posteriormente** al proceso de entrenamiento sobre la época.\n",
    "**IMPORTANTE:** No olvide copiar los batchs al dispositivo de trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_loop(\n",
    "    model, train_dataloader, valid_dataloader, loss_fn, optimizer, epochs\n",
    "):\n",
    "    train_avg_losses_training, train_avg_losses, valid_avg_losses = [], [], []\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs+1)):\n",
    "        tqdm.write(f\"Epoch {epoch}\")\n",
    "        train_avg_loss_training = train_step(model, train_dataloader, loss_fn, optimizer)\n",
    "\n",
    "        train_avg_loss = validate_step(model, train_dataloader, loss_fn)\n",
    "        valid_avg_loss = validate_step(model, valid_dataloader, loss_fn)\n",
    "\n",
    "        tqdm.write(f\"Train avg loss: {train_avg_loss:.6f}\")\n",
    "        tqdm.write(f\"Valid avg loss: {valid_avg_loss:.6f}\")\n",
    "        tqdm.write(\"----------------------------------------------------------------\")\n",
    "\n",
    "        train_avg_losses_training.append(train_avg_loss_training)\n",
    "        train_avg_losses.append(train_avg_loss)\n",
    "        valid_avg_losses.append(valid_avg_loss)\n",
    "\n",
    "    print(f\"Training finished! Trained for {epoch} epochs.\")\n",
    "    print(\n",
    "        f\"Final results from epoch {epoch}:\\n\"\n",
    "        f\"  - Train avg loss: {train_avg_loss:.6f}\\n\"\n",
    "        f\"  - Valid avg loss: {valid_avg_loss:.6f}\"\n",
    "    )\n",
    "    return model, train_avg_losses_training, train_avg_losses, valid_avg_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9)** Entrene y valide el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Junto todo acá de nuevo. Más fácil de ver los parámetros sin tener que scrollear\n",
    "train_set = CustomDataset(train_set_orig)\n",
    "valid_set = CustomDataset(valid_set_orig)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True , num_workers=10, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=10, pin_memory=True)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "p = 0.2\n",
    "model = ConvAutoencoder(p=p)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "learning_rate = 10e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, train_avg_losses_training, train_avg_losses, valid_avg_losses = train_validate_loop(\n",
    "    model, train_loader, valid_loader, loss_fn, optimizer, epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10)** Use las listas del inciso **8)** para graficar en función de las **épocas de entrenamiento** el **ECM** de **entrenamiento** y **validación**, respectivamente.\n",
    "Discuta y comente, cual es el número óptimo de épocas de entrenamiento?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def losses_plot(\n",
    "    train_avg_losses_training, train_avg_losses, valid_avg_losses\n",
    "):\n",
    "    epochs = len(train_avg_losses)\n",
    "    epochs_range = range(epochs)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs_range, train_avg_losses_training, label=\"Durante la época\", linestyle=\"--\", color=\"blue\")\n",
    "    plt.plot(epochs_range, train_avg_losses, label=\"Entrenamiento\", linestyle=\"-\", color=\"green\")\n",
    "    plt.plot(epochs_range, valid_avg_losses, label=\"Validación\", linestyle=\":\", color=\"red\")\n",
    "    \n",
    "    plt.title(\"Pérdidas durante el entrenamiento\")\n",
    "    plt.xlabel(\"Épocas\")\n",
    "    plt.ylabel(\"Pérdida promedio por lote\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # experiment_path = os.path.join(EXPERIMENTS_PATH, f\"experiment_{timestamp}\")\n",
    "    # os.makedirs(experiment_path, exist_ok=True)\n",
    "\n",
    "    # with open(os.path.join(experiment_path, \"config.json\"), \"w\") as f:\n",
    "    #     json.dump(config, f)\n",
    "\n",
    "    # results = {\n",
    "    #     \"train_accuracy\": round(train_accs[best_epoch], 4),\n",
    "    #     \"train_avg_loss\": round(train_avg_losses[best_epoch], 4),\n",
    "    #     \"valid_accuracy\": round(valid_accs[best_epoch], 4),\n",
    "    #     \"valid_avg_loss\": round(valid_avg_losses[best_epoch], 4),\n",
    "    #     \"last_epoch\": best_epoch\n",
    "    # }\n",
    "    # with open(os.path.join(experiment_path, \"results.json\"), \"w\") as f:\n",
    "    #     json.dump(results, f)\n",
    "\n",
    "    # fig.savefig(os.path.join(experiment_path, \"metrics.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_plot(train_avg_losses_training, train_avg_losses, valid_avg_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11)** Grafique, comparativamente, algunas de las imagenes a predecir vs las imagenes predichas por el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_orig_predicted(trained_model, train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12)** Repita para otras elecciones de los hiperparámetros tales como, el optimizador (podría ser el **SGD**), el **learning-rate**, el tamaño de los **batchs**, el **dropout**, **capas convolucionales** y **convolucionales traspuestas** de otros tamaños.\n",
    "En particular, pruebe eliminando, adecuadamente, la **capa lineal**.\n",
    "Que valores de estos hiperparámetros considera los más convenientes? Porqué?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyuXv-0x29Xw"
   },
   "outputs": [],
   "source": [
    "# 5.1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "11i9OWd_kPdEiEyWdwL__9m09UM7jCdy7",
     "timestamp": 1732223079596
    }
   ]
  },
  "kernelspec": {
   "display_name": "TrabajoFinal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
